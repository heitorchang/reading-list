General notes

pd.Series([2, None]) results in [2.0, NaN]. NaN is of type numpy.float64, displayed as nan.
It may be referenced as np.nan and in Python, float('nan')


53 in Jupyter, run "%matplotlib inline" to integrate it properly

58 isinstance(a, int)
58 isinstance(a, (int, float))

59 getattr(a, 'split')

66 o caractere \ é um "caractere de escape". barras invertidas precisam ser escapadas.

71 datetime_obj.replace(minute=0, second=0)

85 to add multiple elements to a list, use .extend() instead of +

88 map items' location in a list

mylist = ['foo', 'bar', 'baz']
mapping = {}
for i, e in enumerate(mylist):
    mapping[e] = i
# mapping: {'bar': 1, 'baz': 2, 'foo': 0}

also (p. 98):
loc_mapping = {val: index for index, val in enumerate(mylist)}

90 convert lines to columns

pitchers = [('Nolan', 'Ryan'), ('Roger', 'Clemens')]
first_names, last_names = zip(*pitchers)

92 mydict.update({'b': 'foo', 'c': 12}) alters mydict in-place

92 dict() accepts a list of tuples (pairs)

93 words = ['apple', 'bat', 'bar', 'atom', 'book']
by_letter = {}
for word in words:
    letter = word[0]
    by_letter.setdefault(letter, []).append(word)

also: defaultdict
by_letter = defaultdict(list)

99 some_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]
flattened = [x for tup in some_tuples for x in tup]
# [1, 2, 3, 4, 5, 6, 7, 8, 9]

compare to:

[[x for x in tup] for tup in some_tuples]
# [[1, 2, 3], [4, 5, 6], [7, 8, 9]]

the order of nested list comprehensions is the same as nested for blocks

100 funções são declaradas com a palavra reservada def, e o retorno é feito com return.

102 funções "devolvem" valores

109 first_letter = lambda x: x[0]
itertools.groupby(names, first_letter)

111 handle multiple exceptions:
try:
    return float(x)
except (ValueError, TypeError):
    return x
 
114 open mode 'x' will fail if the file already exists

Ch. 4 NumPy

122 ndarray é um objeto array N-dimensional

123 data = np.random.randn(2, 3)  # generates 2 rows and 3 columns of random numbers

simply write "data * 2" or "data + data"

data.shape()  # (2, 3)
data.dtype  # dtype('float64')
data.ndim  # 2

124 creating ndarrays

data1 = [6, 7.5, 8, 1, 0]
arr1 = np.array(data1)

125 np.zeros(10)
np.zeros((3, 6))
np.empty((2, 3, 2))  # WARNING: might contain junk values

np.arange(15) is the analogous function to regular "range"

126 np.asarray(arr) does not make a copy if arr is already a ndarray

128 cast an array:
float_arr = arr.astype(np.float64)

WARNING: the dtype numpy.string_ and unicode_ have fixed length and data may be truncated (for example, when an element is changed in-place)

129 bulk, element-by-element arithmetic without loops is called "vectorization"

131 array slices are views (data is not copied)

132 to copy a slice: arr[5:8].copy()

132 in a 2-d array, arr[0][2] is the same as arr[0, 2]. the first index is the row and the second index is the column

134 n-dimensional arrays can be sliced: arr[:2, 1:]. Use the comma syntax [:2, :1]

136 boolean indexing: data[names == 'Bob'] WARNING: if the boolean array dimension does not match the target array, no error is raised

138 ~ (not), & and | can be used to combine boolean conditions. "and", "or" will not work
WARNING: copies are always made with boolean indexing

139 data[data < 0] = 0

140 fancy indexing: get a subset of an ndarray by explicitly passing the desired order:
arr[[4, 3, 0, 6]]
arr[[-3, -5, -7]]

with multiple lists of indices, elements corresponding to each tuple of indices are returned
arr[[1, 5, 7, 2], [0, 3, 1, 2]]
will select [1, 0], [5, 3], [7, 1], [2, 2]

arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]] returns a rectangular subset

fancy indexing always returns a copy of the data

142 arr.T shows the transpose, without copying data

143 arr.transpose((1, 0, 2)) permutes multi-dimensional arrays (also a view)

arr.swapaxes(1, 2)

144 ufuncs (universal functions) operate on all elements of ndarrays

np.sqrt(arr)

146 some ufuncs:
unary: abs, fabs, sqrt, square, exp, log, sign, ceil, floor, rint, modf, isnan, isfinite, logical_not, sin, cos, tan, sinh, arcsinh
binary: add, subtract, multiply, divide, floor_divide, power, maximum, minimum, fmax, fmin, mod, copysign, greater, greater_equal, less,
less_equal, equal, not_equal, logical_and, logical_or, logical_xor

147 points_x = np.arange(-5, 5)
points_y = np.arange(-10, 12)
xs, ys = np.meshgrid(points_x, points_y)

149 np.where is "x if condition else y" in vectorized form
result = np.where(cond, xarr, yarr)
np.where(arr > 0, 2, arr)

150 simple statistics: sum, mean, std

arr.mean(axis=1)  # mean by columns

cumsum, cumprod

153 (arr > 0).sum()  # count number of positive values

bools.any()
bools.all()

0 is falsy for numeric arrays

153 arr.sort()

an axis may be passed: arr.sort(1)

154 5% quantile:
large_arr.sort()
large_arr[int(0.05 * len(large_arr))]

154 np.unique(names)

155 np.in1d(values, [2, 3, 6]) tests if elements of values are in [2, 3, 6]

other set methods: intersect1d, union1d, settdiff1d, setxor1d

156 np.save('some_array', arr)
np.load('some_array.npy')
np.savez('array_archive.npz', a=arr, b=arrb)
np.savez_compressed('array_archive.npz', a=arr, b=arrb)

157 np.dot(x, y) is the same as x.dot(y) and x @ y

numpy.linalg contains standard matrix operations

common linalg functions: diag, dot, trace, det, eig, inv, pinv, qr, svd, solve, lstsq

159 random number generators
np.random.randn(2, 3)
np.random.normal(size=(4, 4))

160 np.random.seed(1223)

rng = np.random.RandomState(1234)
rng.randn(10)

functions: seed, permutation, shuffle, rand, randint, randn, binomial, normal, beta, chisquare, gamma, uniform

163 argmax: returns the first index of the array's max value

165 pandas, like numpy, prefers vectorized operations (without the use of for loops)

pandas was designed for heterogeneous data (different types), while numpy is more appropriate for homogeneous data

import pandas as pd
from pandas import Series, DataFrame

166 a Series is a 1-dimensional sequence of values and an associated array of labels (rótulos), called the index.
pd.Series([-4, 6, 2, 1])

the default index is [0..N-1]

ser.values  # gets data
ser.index  # gets array of indices

an arbitrary index list may be passed when constructing a Series

167 pd.Series([4, 2, 5], index=['b', 'c', 'a'])

indices may be used to retrieve specific values
obj['a']
obj[['b', 'c']]

filtering works like in numpy: obj[obj > 0]

168 a Series behaves like a dictionary

obj3 = pd.Series({"Ohio": 2340, "Michigan": 5200})

169 you may pass a list of indices as a keyword argument to Series to override the order of the dictionary's keys
obj4 = pd.Series(dictdata, index=["Michigan", "Ohio"])

the index array passed will determine the resulting contents (not including an index results in it not showing up)

NaN is the special value for missing data

pd.isnull(obj)
pd.notnull(obj)  # returns True or False for each value

also: obj.isnull(), obj.notnull()

170 arithmetic operations line up objects by index. it is like doing a SQL join

171 obj.name = 'population'
obj.index.name = 'state'

the index may be assigned in-place

obj.index = ['a', 'c', 'b']

171 a DataFrame represents a rectangular table of data and each column may contain a different type of data (string, boolean, number).
there is an index for rows and for columns.

a DataFrame is like a dictionary of Series which share the same index.

172 one of the most common ways of constructing a DataFrame is by passing a dictionary of lists (they must be of the same length).
an list of indices may be passed

df.head() shows the first 5 lines. df.tail() shows the last 5.

173 passing the keyword argument columns will order the DataFrame.
pd.DataFrame(data, columns=['year', 'state', 'population'])

NaN will appear if a column is passed and there is no associated data

a Series is returned when accessing a column via bracket notation: df['population']
dot notation also works for names that would be acceptable variable names: df.population

174 the special attribute loc is used to retrieve rows by passing an index
frame.loc['three']

columns may be reassigned: frame['debt'] = 16.5
frame['year'] = np.arange(6)

175 assigning a Series will align its indices to the indices in the DataFrame
val = pd.Series([1.2, 1.5, 1.7], index=['two', 'one', 'three'])
frame['debt'] = val

NaN will appear when the Series does not specify an index in the DataFrame. extra indices in the Series do not appear.

176 del frame['eastern'] removes that column

a column obtained with indexing is a view. use copy() to make a copy

176 a nested dictionary passed to pd.DataFrame() will transform the outer keys as columns and inner keys as indices

debt = {"Nevada": {2001: 0.4, 2002: 2.9},
        "Ohio": {2000: 1.5, 2001: 0.9, 2002: 3.5}}

177 frame.T transposes the DataFrame

the index keyword may be used when creating a DataFrame

pd.DataFrame(debt, index=[2001, 2002, 2003])

the index list that is passed will override the data's indices

the index and columns may have names

frame.index.name = 'year'
frame.columns.name = 'state'

178 various ways to pass data to construct a DataFrame:

- ndarray (2-D), list of lists, list of tuples: a matrix of data
- dictionary (of arrays, lists or tuples), structured numpy array: each sequence becomes a column
- dictionary of Series: indices are joined
- dictionary of dictionaries: each inner dictionary becomes a column and inner keys become row indices
- list of dictionaries or Series: each item becomes a row; the union of dict keys and Series indices become column names
- another DataFrame: the indices of that DataFrame are used, unless different indices are passed
- numpy MaskedArray: like ndarray (2-D), but masked values become NA/missing

df.values returns the data. if types are different, a dtype that accommodates all data will be chosen (such as dtype=object)

179 obj.index returns an Index. they are immutable

180 obj.columns is also an Index

elements may be duplicated in an Index

index methods: append, difference, intersection, union, isin, delete, drop, insert, is_monotonic, is_unique, unique

182 obj.reindex(['a', 'b', 'c']) assigns a new index

obj.reindex(range(6), method='ffill')  # forward fills missing values

183 reindex(columns=['a', 'b', 'c']) reassigns the columns

frame.loc[['a', 'b', 'c'], ['Texas', 'Utah']] to reindex appears to be deprecated

184 reindex arguments: index, method, fill_value, limit (to fill), tolerance (inexact comparisons),
level (correspondence between a simple index and a MultiIndex level), copy

185 data.drop(['Colorado', 'Ohio']) removes rows (axis 0)
data.drop('two', axis=1)  # removes column named 'two'
data.drop(['two', 'four'], axis='columns')  # removes column named 'two' and 'four'

186 obj.drop('c', inplace=True)

187 a list of indices may be used to retrieve values from a Series
s[['b', 'd']]
s[s < 2]

indexing with labels will include the endpoint:
s['a':'c'] returns a, b, c

188 indexing a DataFrame is used to retrieve columns
df[['one', 'three']]

however, passing a slice will return lines (df[:2])

df[df['three'] > 5]

190 data.loc[] uses indices
data.loc['Colorado', ['two', 'three']]

data.iloc uses integer indices
data.iloc[2, [3, 0, 1]]

slicing is allowed for indices
data.loc[:'Utah', 'two']

data.iloc[:, :3][data.three > 5]

191 various ways of using loc and iloc (iloc and iat accepts integers)

- df[val] selects a column or columns if a slice is used
- df.loc[val] selects line(s)
- df.loc[:, val] selects column(s)
- df.loc[val1, val2] selects lines and columns
- df.iloc[where] selects lines by position
- df.iloc[:, where] selects column(s)
- df.iloc[where_i, where_j] selects lines and columns
- df.at[label_i, label_j] selects a single value by index 
- df.iat[i, j] selects a single value by position
- reindex selects lines or columns by labels
- get_value, set_value selects a single value by the line and column labels

193 use loc for labels, and iloc for integer indices

193 arithmetic with two DataFrames produces a result with indices from both DataFrames.
wherever there are missing values, the result will be NaN

195 when there are no rows or columns in common, the result will be all NaN.

197 df1.add(df2, fill_value=0) will put fill_value instead of NaN.

functions with 'r' in the front invert the arguments

fill_value can also be used in reindexing
df1.reindex(columns=df2.columns, fill_value=0)

198 arithmetic: add, sub, div, floordiv, mul, pow
radd, rsub, rdiv, ...

198 broadcasting refers to behavior of arithmetic between arrays of different shapes.
values of the smaller array are "broadcast" to fill in missing parts of the larger array

199 by default, arithmetic between a DataFrame and Series will broadcast by lines.
frame - series

200 to broadcast by columns:
frame.sub(series, axis='index')  # or axis=0

200 numpy ufuncs work with pandas objects

201 frame.apply() takes a function to be applied for every column or line
f = lambda.x: x.max() - x.min()
frame.apply(f)  # returns a value for each column
frame.apply(f, axis='columns')  # returns a value for each line

202 common aggregation functions like sum and mean already exist, use apply() for custom functions

apply can also return a Series

def f(x):
    return pd.Series([x.min(), x.max()], index=['min', 'max'])

frame.apply(f)

frame.applymap(f)  # applies to each element

203 obj.sort_index()

to sort by column: frame.sort_index(axis=1)

to sort descending: frame.sort_index(ascending=False)

204 obj.sort_values()
NaN appears at the end (regardless of ascending=True or False)

sort a DataFrame by column: frame.sort_values(by='b')

sort a DataFrame by multiple columns: frame.sort_values(by=['b', 'a'])

205 obj.rank() assigns a ranking to each value (equal values are assigned the average of those values)

obj.rank(method='first') ranks by the order of observation

206 ascending=False may be passed to rank()

207 rank accepts a method keyword parameter to break ties. available methods are: average, min, max, first, dense (ranks always increase by 1)

207 obj.index.is_unique checks if all values are unique

207 if you have duplicate values in the index, you will get all values/rows/columns that match

208 reductions and summary statistics functions return a single value for a Series

209 df.sum() adds the total for each column
df.sum(axis='columns') adds totals for each row

passing skipna=False will cause sum() to return NaN if there's a NaN in the Series

210 passing the keyword argument level will group by level (in a MultiIndex axis)

idxmin() and idxmax() return the index label of the min/max value. argmin, argmax return the integer index.

cumsum() accumulates the running total

df.describe() produces a summary table

211 descriptive statistics: count, describe, min, max, argmin, argmax, idxmin, idxmax, quantile, sum, mean, median, mad, prod, var, std, skew,
kurt, cumsum, cummin, cummax, cumprod, diff, pct_change

212 pandas-datareader contains convenience functions like get_data_yahoo(ticker) (stock prices)

213 obj.corr() and obj.cov() compute correlation and covariance.

obj.corrwith() accepts a parameter and computes correlations of pairs of data

214 obj.unique() returns an ndarray of unique values

obj.value_counts() returns the frequency of each value. sort=False may be passed

215 obj.isin(['b', 'c']) checks if each element of obj is contained in ['b', 'c'] and returns True or False for that value

mask = obj.isin(['b', 'c'])
obj[mask]  # filters obj, keeping only 'b's and 'c's

216 Index.get_indexer(to_match) returns an array of indices of an array of values (with possible duplicates)

to_match = pd.Series(['c', 'a', 'b', 'b', 'c', 'a'])
unique_vals = pd.Series(['c', 'b', 'a'])
pd.Index(unique_vals).get_indexer(to_match)
# array([0, 2, 1, 1, 0, 2])

match (? method does not exist): computes integer indices for each element of an array in another array of unique values

217 data.apply(pd.value_counts).fillna(0) produces a histogram for each column (row index labels are the values counted)

218 common functions to read data:
read_csv
read_table: tab-separated text file
read_fwf: fixed-width columns
read_clipboard
read_excel
read_hdf: binary format
read_html
read_json
read_msgpack: binary format
read_pickle: Python pickle
read_sas
read_sql
read_stata
read_feather

219 optional parameters passed to these functions:

indexing: whether column names should be obtained from the file, user, or neither
data conversion and handling of null values
datetime parsing
iteration over large files
handling "dirty" data

220
# ex1.csv
a,b,c,d,message
1,2,3,4,hello
5,6,7,8,world
9,10,11,12,foo

221 header=None indicates that the source does not contain column names. the result will have a simple range produced by pandas
names=['a', 'b', 'c'] sets the columns to 'a', 'b', 'c'

passing index_col='message' will set the values in the 'message' column as the index for the rows.

222 pd.read_csv('csv_mindex.csv', index_col=['key1', 'key2']) reads a hierarchical index of multiple columns

# csv_mindex.csv
key1,key2,value1,value2
one,a,1,2
one,b,3,4
one,c,5,6
one,d,7,8
two,a,9,10
two,b,11,12
two,c,13,14
two,d,15,16

parsed = pd.read_csv('csv_mindex.csv', index_col=['key1', 'key2'])

           value1  value2
key1 key2                
one  a          1       2
     b          3       4
     c          5       6
     d          7       8
two  a          9      10
     b         11      12
     c         13      14
     d         15      16

parsed['value1']  # returns column 'value1'
parsed.loc['one']  # returns first block
parsed.loc['one', 'a']  # returns first row, [1, 2]

223 pd.read_table('ex3.txt', sep='\s+')  # uses whitespace as the separator

skiprows=[0, 2, 3]  # ignores the rows with the given indices

224 the empty string, NA and NULL are common sentinels that indicate a missing value

na_values=['NULL'] specifies the null sentinel values

sentinel values by column may be specified:
sentinels = {'message': ['foo', 'NA'], 'something': ['two']}
pd.read_csv('ex5.csv', na_values=sentinels)

225 common arguments for reading csv and tables
path
sep / delimiter
header
index_col
names (column names)
skiprows
na_values
comment (character)
parse_dates (if True, attempts parsing all columns. a list of column indices or names may be passed)
keep_date_col
converters
dayfirst (convention for date format, default=False)
date_parser
nrows (read only this many rows from the beginning)
iterator
chunksize (iterate over this many rows, to read file in parts)
skip_footer
verbose
encoding
squeeze
thousands (character for thousands separator)

226 limit display: pd.options.display.max_rows = 10

228 data.to_csv(filename, sep="|", na_rep="NULL", index=False, header=False)  # writes a csv file
the argument columns=['a', 'b', 'c'] selects a subset of columns

230 in some cases, manual processing of a delimited file is necessary:

# ex7.csv
"a","b","c"
"1","2","3"
"1","2","3"

with open('ex7.csv') as f:
    lines = list(csv.reader(f))
    header, values = lines[0], lines[1:]
    data_dict = {h: v for h, v in zip(header, zip(*values))}
# {'a': ('1', '1'), 'b': ('2', '2'), 'c': ('3', '3')}

csv.Dialect may be subclassed to define the csv's properties

class my_dialect(csv.Dialect):
    lineterminator = '\n'
    delimiter = ';'
    quotechar = '"'
    quoting = csv.QUOTE_MINIMAL
reader = csv.reader(f, dialect=my_dialect)

keyword parameters may also be passed instead of defining a dialect subclass

common options: delimiter, lineterminator, quotechar, quoting, skipinitialspace, doublequote, escapechar

232 csv.writer is used to create files

with open('mydata.csv', 'w') as f:
    writer = csv.writer(f, dialect=my_dialect)
    writer.writerow(('a', 'b', 'c'))
    writer.writerow(('1', '2', '3'))

232 basic JSON types are: objects (dicts), arrays (lists), strings, numbers, booleans and null (None).
all keys must be strings

232 json.loads(jsonstr) converts a string to a Python object

233 json.dumps(pyobj) converts a Python object to a JSON string

passing a list of dictionaries (that was previously a JSON) to DataFrame() creates a row for each dictionary.

pd.read_json() converts an array of objects (dictionaries) to a DataFrame

data.to_json() returns a JSON string, with column names as keys and objects containing values for that column
data.to_json(orient='records') produces an object for each row.

234 read_html depends on lxml, beautifulsoup4 and html5lib (run pip install)

236 XML (SKIPPED)

239 frame.to_pickle(filename)
pd.read_pickle(filename)

avoid pickle for long-term storage (format or libraries may change)

240 HDF5

frame = pd.DataFrame({'a': np.random.randn(100)})
store = pd.HDFStore('mydata.h5')
store['obj1'] = frame
store['obj1_col'] = frame['a']

HDFStore may store data as 'fixed' or 'table'. 'table' is slower but allows a special lookup syntax

store.put('obj2', frame, format='table')
store.select('obj2', where=['index >= 10 and index <= 15'])

store.close()

put is the explicit version of store['obj2'] = frame, and allows passing options like format

frame.to_hdf('mydata.h5', 'obj3', format='table')
pd.read_hdf('mydata.h5', 'obj3', where=['index < 5'])

WARNING: if storing data in remote servers like Amazon S3 or HDFS, a different format like Apache Parquet may be more appropriate.
HDF5 is not a database, and may be corrupted if many writers operate on it simultaneously.

242 xlsx = pd.ExcelFile(filename)  # returns an ExcelFile instance.
to read XLS, xlrd is needed and XLSX needs openpyxl.

pd.read_excel(xlsx, 'Sheet1')

passing the ExcelFile instance is faster for reading many sheets, but the filename can be passed directly

to write Excel files:
writer = pd.ExcelWriter('ex2.xlsx')
frame.to_excel(writer, 'Sheet1')

a filename may be passed instead of an ExcelWriter instance.

after working on the Excel file, close the ExcelFile and ExcelWriter.

244 Web APIs

import requests
url = "https://api.github.com/repos/pandas-dev/pandas/issues"
resp = requests.get(url)
data = resp.json()
data[0]['title']  # 'BUG: rank with +inf'

issues = pd.DataFrame(data, columns=['number', 'title', 'labels', 'state'])

245 SQLite interaction (SKIPPED)

consider using SQLAlchemy

249 the sentinel value for nonexistent numerical data is NaN. the isnull() method checks a Series or DataFrame for NaN.
missing data is labeled NA (not available).
None is treated as NA.

250 notnull is the opposite of isnull.
dropna discards values or rows with NA or NaN. a tolerance may be passes (how='all' discards rows with all NA).
fillna replaces NA with a given value.

252 data.dropna(axis=1, how='all') discards columns that are all NA.

253 df.dropna(thresh=2) keeps rows with 2 or more valid values

253 passing a dict to fillna specifies what value to use for each column
df.fillna({1: 0.5, 2: 0})  # the second column gets filled with 0.5, the third with 0

255 fillna arguments:
  value: alternative value used to fill NA values
  method: interpolation method, by default 'ffill'
  axis: default is 0, use 1 for columns
  inplace: if True, mutates the object
  limit: how many consecutive values to fill

256 data.duplicated() returns True or False for each row, if it has been seen before

data.drop_duplicates() removes identical rows, but keeps the first occurrence.
passing keep='last' will keep the last occurrence.

drop_duplicates([column_name]) will only check the given column(s)

the Series method map(value_dict) may be used to associate a column with another

city_to_state = {
    'LA': 'CA',
    'SF': 'CA',
    'NY': 'NY',
    'CHI': 'IL',
    'SD': 'CA'
}

df = pd.DataFrame({"city": ['LA', 'SF', 'CHI'], "students": [3, 1, 2]})
df['state'] = df['city'].map(city_to_state)

260 data.replace(a, b)
data.replace([a1, a2], [b1, b2])
data.replace({-999: np.nan, -1000: 0.0})
inplace=True may be passed

262 data.index.map(lambda x: x[:3].upper())

data.rename(index=str.title, columns=str.upper)
inplace=True may be passed
