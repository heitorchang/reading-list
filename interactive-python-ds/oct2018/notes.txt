https://runestone.academy/runestone/static/pythonds/index.html

Problem Solving with Algorithms and Data Structures using Python

1.1 Problem solving, abstraction, abstract data types

1.2 Review the framework within which computer science and the study of algorithms and data structures must fit

1.3 Computers are just tools

An algorithm is a step-by-step list of instructions for solving any instance of a problem that might arise. They are finite and give solutions.

A problem is computable if there is an algorithm to solve it. Computer Science studies both solvable and unsolvable problems

Abstraction separates logical and physical perspectives. For example, the functions of a car from a logical perspective is braking, steering and accelerating. Such functions are called the interface. The user is called the client.

The car's physical perspective is of interest to a mechanic. He must know how the engine works (details "under the hood").

Procedural abstraction is separating the interface from its implementation. We can call math.sqrt without knowing how it works.

1.4 Programming is the process of taking an algorithm and encoding it into a language's notation that is understood by a computer.

Control constructs such as sequential processing, decision making and iteration allow representation of algorithmic steps unambiguously

Data types provide an interpretation of raw binary data. They are also called "Primitive data types" (for example, integers)

1.5 Abstractions help manage the complexity of problems.

An Abstract Data Type (ADT) is a logical description of how we view the data and apply operations without regard to how it will be implemented.

This level of abstraction creates an encapsulation around the data (also called information hiding)

The implementation (physical view) of an ADT is a data structure

The separation of logical and physical views allow an implementation-independent view of the data

1.6 By seeing how algorithms are designed, we can develop pattern recognition.

There will often be trade-offs we need to identify and decide upon.

1.7 Python is an interpreted, object-oriented programming language. It considers data to be the focal point of the problem-solving process

1.8 A class is a description of what the data looks like (the state) and what they can do (the behavior). A class is analogous to an Abstract Data Type.

Data items are called objects (instances of a class)

1.8.1 Built-in Atomic Data Types

numeric: int, float
arithmetic: +, -, *, /, **, %, //
boolean: True, False, and, not
<, >, <=, >=, ==, !=

Assignment (theSum = 0) associates a name with a value. The variable will hold a reference to a piece of data and not the data itself.

(StackOverflow) ints are immutable, so when changing the value of a local variable (inside a function), a new integer object is created.

Re-assignment inside a function moves the reference to a new object, while changes to a mutable object (alist[0] = 0) does not change the alist reference itself.

Python is dynamic: the same variable can refer to many different types of data

1.8.2 Built-in Collection data Types

Ordered: List, String, Tuple
Unordered: Dictionary, Set

Lists are heterogeneous (elements need not be of the same class)

List Operations
indexing      [i]
concatenation +
repetition    *
membership    in
length        len()
slicing       [a:b:step]

Indices start counting with 0. Slice index a is included, but b is not.

When repeating a list, the references are copied. So a repeated nested list is the same list. Changing it in one place will affect all other copies.

List methods

alist.append(item)
.insert(i, item) inserts item into the ith position
.pop(), .pop(i)
.sort(), .reverse()
.index(item)
.count(item)
.remove(item) removes the first occurrence of item
del alist[i]

list(range(5, 10, 2)) => [5, 7, 9]

String methods
(sequence operations also work)
Alignment: center, ljust, rjust
Case: upper, lower, swapcase, title
count, find, rfind, index, rindex

find vs. index
s.index('a') will raise ValueError if it doesn't find the argument
find returns -1
Mnemonic: -1 is not found, you can't say not index. Also, -1 is the last character.

split uses the argument as division points or whitespace if there are no arguments.

Lists are mutable, strings and tuples are not.

Sets do not allow duplicates.
The empty set is set()
a | b Union (all elements from both sets)
a & b Intersection (elements common to both)
a - b Difference (elements in a but not in b)
a <= b Asks whether all elements of a are in b (isSubset)

add, remove, pop, clear
pop removes an arbitrary element from the set

Dictionaries are collections of associated pairs of keys and values

capitals = {'Iowa': 'Des Moines', 'Wisconsin', 'Madison'}
capitals['California'] = 'Sacramento'

list(adict) returns the keys

keys, values, and items return objects. They may be converted to lists.

adict.get('akey', 'not found') returns the value associated to 'akey' or 'not found' if that key is not present. Without the second argument, .get() returns None if the key is not found.

1.9

aName = input('Please enter your name: ')
input() returns a string
in Python 2, use raw_input()

use float() or int() to convert to numbers

print(a, b, c, sep="***", end="!!\n")

formatted string
print("%s is %d years old." % (aName, age))

there is a .format() method not covered in this book

1.10 Control Structures
counter = 1
while counter <= 5:
    print(counter, end=" ")
	counter += 1

=> 1 2 3 4 5

and, or are short-circuit operators

for item in [1, 3, 6, 5, 0]:

for i in range(5):

for aletter in aword:

if ... else
if ... elif ... elif ... else

[Internet] 'else' at the end of for and while loops are executed if the loop completes (is not interrupted by break, for example)

list comprehension: add 10 to odd integers
[x + 10 for x in range(10) if x % 2 == 1]
=> [11, 13, 15, 17, 19]

a = [1, 1, 0]
['T' if n == 1 else 'F' for n in a]
=> ['T', 'T', 'F']

1.11 Exception Handling

A syntax error occurs when there's a mistake in the structure of a statement or expression

[PyRef] A simple statement is comprised within a single logical line

Expression statements are mostly used interactively, to compute and write a value or to call a procedure

a logic error occurs when the program executes but gives the wrong result. If a situation such as division by zero or list access out of bounds occurs at runtime, it is called an exception

Exceptions are raised and handled

try ... except blocks are used to handle potential exceptions. The except block "catches" the exception.

The programmer can raise his or her own exceptions

raise RuntimeError("can't use negative numbers")

1.12 Defining Functions

A function hides the details of a computation

A function definition requires a name, a group of parameters, and a body. It may explicitly return a value

(an argument is a value given to a function; try 's'.join(). It raises TypeError: join takes exactly one argument (0 given))

Newton's method for square roots:
newguess = (1/2) * (oldguess + (n / oldguess))

1.13 Defining classes

A programmer may create new classes to model data that is needed to solve the problem.

To implement an abstract data type, we use a new class.

1.13.1 Fraction

3 (numerator)
- 
5 (denominator)

class Fraction:
    def __init__(self, top, bottom):
	    self.num = top
		self.den = bottom

self is a special parameter that is used as a reference back to the object itself.

'self' must always be the first formal parameter

To create an instance, we invoke the constructor:
myFraction = Fraction(3, 5)

we want to override the __str__ method to get an useful result with print().

[Fluent Py] __repr__ should be "as-code"
    return "Fraction({}, {})".format(self.num, self.den)

override __add__ method

Euclid's algorithm for GCD
[sicp]

def gcd(a, b):
    if b == 0:
	    return a
	return gcd(b, a % b)

In this Fraction implementation, a negative fraction will have a negative numerator.

shallow equality is when two references (variables) refer to the same object

deep equality is when two distinct objects have the same value (not the same reference). __eq__ and __le__ may be overridden.

1.13.2 Inheritance: Logic gates and circuits

Inheritance is the ability for one class to be related to another. Child classes can inherit characteristic data and behavior from a parent class

Child classes are called subclasses and parents are called superclasses. A child and parent follow a "is-a relationship".

The most general class for the digital circuit simulation is LogicGate

The general LogicGate class uses performGateLogic, which will be implemented by subclasses.

class UnaryGate(LogicGate):
	def __init__(self, n):
    	super().__init__(n)
		# Or super(UnaryGate, self).__init__(n)
		LogicGate.__init__(self, n)

Child class constructors need to call parent class constructors and then move on to their distinguishing data.

The connector class is not part of the gate hierarchy, but follows a "has-a" relationship. A connector has two gates. "Has-a" does not involve inheritance.

2.1 Analysis

Big-O execution time of common list and dictionary operations

2.2 How do we decide one program is better than another?

There may be many programs for the same algorithm

Algorithm analysis is concerned with comparing the amount of computing resources (time and memory) each algorithm uses

Benchmarking actual time to run an algorithm is not really useful because it is dependent on a particular machine, program, and language

def sumOfN(n):
    start = time.time()
	theSum = ...
	end = time.time()
	return theSum, end - start

2.3 Big-O notation

To characterize efficiency independent of a particular program or computer, it is important to quantify the number of operations.

For a function T(n) denoting the number of operations (such as assignment), the most dominant part is the "order of magnitude" function, O(f(n)). f(n) is a simple representation of the dominant part of T(n).

For algorithms that depend on the exact values of the data, we may characterize their performance in terms of best case, worst case, or average case

common orders of magnitude are:

f(n)     name
1        constant
log n    logarithmic
n        linear
n log n  log linear
n^2      quadratic
n^3      cubic
2^n      exponential
n!       factorial

2.4 Check if two strings are anagrams

2.4.1 "check off" characters from the first string by replacing characters in a list (converted from the second string) with None as they are seen. It is O(n^2)

2.4.2 Sort and compare
Sort both strings alphabetically and compare them. Its time complexity depends on the sorting algorithm used, typically O(n log n)

2.4.3 Brute force (exhaust all possibilities)
There are n characters for the first position, n-1 for the second, and so on. It is O(n!)

2.4.4 Count and compare
Use a list of 26 counters, each for a possible letter. Then compare the two lists. This algorithm is O(n) but requires additional storage. It sacrifices space in order to gain time

2.5 Performance of Python Data Structures
(lists and dictionaries)

2.6 Lists

# timeit.Timer
def test1():
    alist = [i for i in range(1000)]

t1 = Timer("test1()", "from __main__ import test1")
t1.timeit(number=100)

indexing and assigning        O(1)
append                        O(1)
concatenate a list of size k  O(k)  # alist += [i]
pop()                         O(1)
pop(i)                        O(n)
insert(i, item)				  O(n)
del							  O(n)
contains (in)  				  O(n)
slice						  O(n)
reverse						  O(n)
sort						  O(n log n)
multiply					  O(nk)

2.7 Dictionaries

You access dictionary items by a key, while in lists, it is done by index (its position)

get			   O(1) \
set            O(1)  > Average performance
contains (in)  O(1) /
copy           O(n)
delete         O(1)
iteration      O(n)

3.1 Objectives
- Cover the abstract data types
    stack, queue, deque, list

- prefix, infix, postfix expression formats
    - use stacks for these expression formats

- use queues for timing simulations

- use the node and reference pattern for linked lists

3.2 Linear data structures are collections where items stay in the same place relative to others that were added or removed.

  left/front [][][][][][] right/rear

3.3 In a stack, addition and removal takes place at the same end--the top

The opposite end is called the base, and represent items that have been there the longest.

Stack ordering is called LIFO (Last-In, First-Out)

By stacking items, then removing them one at a time to a new stack, order is reversed

Analogy: Browser history. The back button removes the newest URL

3.4 The Stack Abstract Data Type

Stack() creates an empty stack
push(item) adds item to the top of the stack
pop() removes and returns the top item
peek() returns the top item but does not remove it
isEmpty() tests whether the stack is empty
size() returns the number of items on the stack

3.5 Implementing a stack

class Stack:
    def __init__(self):
	    self.items = []

	def isEmpty(self):
	    return self.items == []

	def push(self, item):
		self.items.append(item)

	def pop(self):
		return self.items.pop()

	def peek(self):
		return self.items[-1]

	def size(self):
		return len(self.items)

Though the implementation could have placed the stack's top to the left end of the 'items' list, the performance will be O(n) instead of O(1)

Example: reverse a string by using a stack

3.6 Simple balanced parentheses

Closing symbols match opening symbols in the reverse order of their appearance.

In the algorithm, we initialize 'balanced' to be True; there is no reason to assume otherwise.

At the end, the stack should be empty

3.7 Balanced brackets

In the general case, we must check that a closing bracket matches the type of the opening bracket on top of the stack

3.8 Converting decimal numbers to any base from 2 to 9
(the text covers binary)

[Rosen p. 249]
def changeBase(n, base):
	digits = []
	while n > 0:
		digits.append(n % base)
		n //= base
	return ''.join(map(str, digits[::-1]))

for bases above 9, we define a string (a list) of valid digits
validDigits = "0123456789ABCDEF"
... digits.append(validDigits[n % base])

3.9 Infix, prefix, postfix

A + B * C is infix, but operators have different precedence levels to remove ambiguity in the order of operations

Prefix notation requires that all operators precede (come before) operands. + A B

Postfix requires that operators come after the operands. A B +. Reverse Polish Notation (RPN) is postfix.

infix         prefix      postfix
(A + B) * C   * + A B C   A B + C *

In order to convert infix to prefix or postfix, fully parenthesize the infix expression, then move the enclosed operator to either the left parenthesis (for prefix) or the right one (for postfix).

In the general algorithm for infix to postfix, we use a stack to store the operators.

1. Create a stack for operators 'opstack'
2. Split the input string (in infix)
3. Scan the token list from left to right
   - if it's an operand, append it to the end of the output list
   - if it's a left paren, push it on the opstack
   - if it's a right paren, pop all operators, appending them to the output, until the corresponding left paren appears.
   - if it's an operator, first remove operators already on the opstack of higher or equal precedence. Then push it on the opstack
4. Any remaining operators can be removed and appended to the end of the output list.

prec = {}  # holds precedence levels of operators
prec['*'] = 3
prec['/'] = 3
prec['+'] = 2
prec['-'] = 2
prec['('] = 1
prec[')'] = 1

3.9.3 Postfix Evaluation

1. Create an empty stack 'operandStack'
2. Split the input string
3. Scan the token list from left to right:
   - if it's an operand, push it on the operand stack
   - if it's an operator, pop the operandStack twice. The first pop is the second operand, and the second pop is the first operand. Push the result back on the operandStack.
4. The result is the value on the operandStack. Pop and return it.

3.10 Queues

A queue is an ordered collection of items where the addition of new items happens at one end, the "rear", and removal at the other end, the "front".

The most recently added item must wait, while the item that has been in the collection the longest is at the front.

This ordering principle is called FIFO (First-In, First-Out)

Examples: print tasks, operating system scheduling

3.11 The Queue Abstract Data Type

Queue() creates a new queue
enqueue(item) adds item to the rear
dequeue() removes and returns the front item
isEmpty() tests to see whether the queue is empty
size() returns the number of items in the queue

q = Queue()       []
q.enqueue(4)      [4]
q.enqueue('dog')  ['dog', 4]
q.dequeue()       ['dog']         => returns 4
              rear          front

3.12 Implementing a queue

# pythonds.basic.queue

class Queue:
    def __init__(self):
	    self.items = []

	def isEmpty(self):
		return self.items == []

	def enqueue(self, item):
		self.items.insert(0, item)

	def dequeue(self):
		return self.items.pop()

	def size(self):
		return len(self.items)

3.13 Hot Potato (Josephus problem)

A queue acts as a circle, where the front person passes the potato to the person behind, is dequeued, and immediately enqueued (going to the rear of the queue). After 'num' passes of the potato, the front person is removed.

3.14 Printing Tasks

Simulate 10 students printing twice a paper of length between 1 and 20 pages.

Create classes for Printer, Task, and PrintQueue, 'tick' to decrement the internal timer.

3.15 What is a deque?

A deque is a "double-ended queue" and is structurally similar to a queue--it has a front and a rear. However, items may be added and removed from either end.

(StackOverflow) A deque is a doubly-linked list

A deque provides all the capabilities of stacks and queues in a single data structure

3.16 The Deque Abstract Data Type

Deque() creates a new deque
addFront(item)     [..., ...]
addRear(item)     rear    front
removeFront()
removeRear()
isEmpty()
size()

collections.deque has:
appendleft()  append()
popleft()     pop()

3.17 Implementing a Deque

...
def addFront(self, item):
    self.items.append(item)  # right end

def addRear(self, item):
    self.items.insert(0, item)  # left end

def removeFront(self):
    return self.items.pop()

def removeRear(self):
    return self.items.pop(0)

3.18 Palindrome Checker

Place word in a deque. While size > 1, remove both the front and rear characters. If they are not the same, the word is not a palindrome

3.19 Lists

A list is a collection of items where each item holds a relative position with respect to the others (first item, second item, third item, and so on). We assume here that lists don't have duplicates.

A list may be ordered or unordered. In an ordered list, the items' values are arranged either in ascending or descending order (sec. 3.22)

3.20 The Unordered List Abstract Data Type

List()
add(item) Adds item as the new head. Assume item is not already there
remove(item) We are assuming there are no duplicates
search(item) returns a Boolean
isEmpty()
size()
append(item) adds item to the end
index(item) assumes the item is in the list
insert(pos, item)
pop() removes and returns the last item
pop(pos)

3.21 Implementing an Unordered List with Linked Lists

The location of the first item of a linked list must be explicitly specified. This external reference is called the "head" of the list.

The first item knows where the second item is. The second knows the third, and so on. The last item needs to know there is no next item.

The basic building block is the "node". It contains the list item itself in the "data field". It also holds a reference to the next node.

class Node:
	def __init__(self, initdata):
		self.data = initdata
		self.next = None

	... getData(), setData()
	getNext(), setNext()

-||| (grounding the node) indicates None, the end of the list

3.21.2 The Unordered List Class

Each list object will maintain a single reference to the head of the list.

mylist   head
  []--->( [] )--->|||

mylist   head
  []--->( [] )--->[54| ]--->[26| ]--->[12| ]--->|||

The list class itself does not contain Node objects. Instead, it contains a single reference to only the first node in the linked structure.

isEmpty() checks if the head is None

Adding items is easiest at the head.

def add(self, item):
	temp = Node(item)
	temp.setNext(self.head)
	self.head = temp

size, search and remove are implemented with "linked list traversal" (systematically visiting each node). An external reference that starts at the first node is used ("current")

For searching, we initialize "found" to False

remove() assumes the item is present. Two external references, "previous" and "current", are needed, because "previous"'s next needs to become "current"'s next. "previous" starts as None

In traversing, "previous" must be moved forward first (this process is called "inch-worming")

If the item to be removed is at the head, a special procedure is needed because "previous" is None. Head is set to current's next.

append, insert, index and pop are not shown.

3.22 The Ordered List Abstract Data Type

in ascending order, a list may be [17, 26, 31, 54, 79, 93]

The relative position of each item is based upon some underlying characteristic of the item

OrderedList()
add(item) makes sure order is preserved
remove(item)
search(item) returns a Boolean
isEmpty(), size()
index(item)
pop()
pop(pos)

3.23 Implementing an Ordered List

initializing, isEmpty, size and remove are the same as Unordered List

In searching, we may take advantage of the ordering to stop after the searched value is passed. For example, searching for 45 in [17, 26, 31, 54, 77, 93] can stop once 54 is reached.

in search(), initialize another Boolean, "stop", to False

when adding an item, the method must decide which two items will surround the new item.

Seeing a value greater than 'item' will cause us to stop.

3.23.1 Analysis of Linked Lists

If traversal is needed (length, size, search, remove) and insert and add in ordered lists, the complexity is O(n)

Adding at the head (in unordered lists) and isEmpty() is O(1)

4.1 Recursion - Objectives

complex problems that may otherwise be difficult to solve may have a simple recursive solution

4.2 What is Recursion?

Recursion is a method of solving problems that involves breaking them down into smaller and smaller subproblems until you get to a small enough problem that can be solved trivially.

Usually recursion involves a function calling itself.

4.3 Calculating the Sum of a List

How would you add a list of numbers without 'for' or 'while' loops?

Addition is defined for a pair of numbers. We may add parentheses and sequentially add numbers

total = (1 + (3 + (5 + (7 + 9))))
      = (1 + (3 + (5 + 16)))
	  = (1 + (3 + 21))
	  = (1 + 24)
	  = 25

listSum(numList) = first(numList) + listSum(rest(numList))

if len(numList) == 1:
	return numList[0]
else:
	return numList[0] + listSum(numList[1:])

The check for a list of size 1 is our "escape clause". The sum of a list of size 1 is trivial: the number in the list

When we reach the point where the problem is as simple as it can get, we begin to piece together the solutions of each of the small problems until the initial problem is solved.

4.4 The 3 laws of recursion

1. A recursive algorithm must have a base case
2. A recursive algorithm must change its state and move toward the base case
3. A recursive algorithm must call itself, recursively

The base case is the condition that allows the algorithm to stop recursing (a trivial case that may be solved directly).

Usually the data gets smaller in some way when a recursive call is made.

A recursive factorial function should have n <= 1 as its base case (the factorial of zero is 1 and the program would not crash with negative inputs)

4.5 Converting an integer to a string in any base

Alternative algorithm to using a stack.

Use integer division to reduce the number and concatenate the remainder.

def convBase(n, base):
	convStr = "0123456789ABCDEF"
	if n < base:
		return convStr[n]
	else:
		return convBase(n // base, base) + convStr[n % base]

4.6 Stack Frames: Implementing recursion

Instead of concatenating the result of the recursive call in the base conversion algorithm, we could use a stack to store the digits.

When a function is called, a stack frame is allocated to handle the function's local variables.

In list summing, you can think of the return value on the stack taking the place of an accumulator variable.

A stack frame also provides a scope for the function's variables. Each call creates a new scope.

4.7 Visualizing recursion

Turtle graphics: move forward, backward, turn left, right, pen up, pen down

draw a spiral by recursively drawing shorter and shorter line segments and turning right

+--------+
| +----+ |
| | +- | |
| | +--+ |
| +------+
 
A fractal has the same basic shape no matter how much you magnify it (snowflakes, coastlines, trees)

In drawing a tree, we make sure to decrease the size of branches so that the recursion ends.

def tree(branchLen, t):  # t is turtle
	if branchLen > 5:
		t.forward(branchLen)
		t.right(20)
		tree(branchLen-15, t)
		t.left(40)
		tree(branchLen-10, t)
		t.right(20)
		t.backward(branchLen)

the tree is drawn all the way to the right first.

4.8 The Sierpinski Triangle is another fractal. It can be drawn with a three-way recursive algorithm. The base case is when the "degree" of the fractal is 0 (we stop recursing)

The algorithm first draws the outer triangle, then three recursive calls, one for each of the corner triangles.

      O
left /|\ right
	O O O
   /|\top
  / | \
 O  O  O
left  right
   top

4.9 Complex Recursive Problems

Some problems that are difficult to solve iteratively may be easily and elegantly solved with recursion.

4.10 Tower of Hanoi
The base case is moving a tower of one disk from one peg to another.

To move a tower of height h:

1. Move a tower of height h - 1 to an intermediate pole, using the final pole
2. Move the remaining disk to the final pole
3. Move the tower of height h - 1 from the intermediate pole to the final pole using the original pole.

def moveTower(height, fromPole, toPole, withPole):
    if height >= 1:
	    moveTower(height-1, fromPole, withPole, toPole)
		moveDisk(fromPole, toPole)
		moveTower(height-1, withPole, toPole, fromPole)

def moveDisk(from, to):
    print("move disk from", from, "to", to)
	
4.11 Exploring a Maze - Finding a way out

First, try going North and recursively try the procedure from there.

If the path is blocked, try South, then West and East

We must remember where we have been (breadcrumbs)

Base cases:

1. The turtle has run into a wall
2. Found a visited square
3. Found an outside edge (exit from the maze)
4. Explored a square unsuccessfully in all 4 directions

class Maze:
	def __init__(self, mazeFileName)
	def drawMaze
	def updatePosition
	def isExit
	def __getitem__(self, idx):  # for maze[i][j]
		return self.mazelist[idx]

isExit checks if row or col are 0 or the last index of rows or cols (and not a wall)

The main algorithm is 'searchFrom'

The base cases are checked:

1. obstacle
2. tried
3. isExit

Otherwise, set found to
	searchFrom(maze, row-1, col) or
	searchFrom(maze, row+1, col) or
	searchFrom(maze, row, col-1) or
	searchFrom(maze, row, col+1)

if found:
	maze.updatePosition(row, col, part_of_path)
else:
	maze.updatePosition(row, col, dead_end)

4.12 Dynamic Programming is a strategy for solving optimization problems

Making change using the fewest coins is a classic problem

The greedy method tries to solve as big a piece of the problem as possible right away. It works for U.S. coins, but not in general (such as including a 21-cent coin and solving for 63 cents)

a recursive solution is too inefficient.

                / 1 + numCoins(originalAmt - 1)  # penny
numCoins = min {  1 + numCoins(originalAmt - 5)  # nickel
                \ ... etc.

The key to cutting down the amount of work done is to store past results (memoization, or caching)

A true dynamic programming algorithm begins at 1 cent and systematically works its way up to the desired amount

From 1 to 4 cents, we can only use pennies. For 5 cents, we store the minimum of 4 pennies plus 1 penny (5 coins) and 0 coins plus 1 nickel (1 coin)

For 11 cents, we consider:

#1 a penny  plus the minimum to make 11-1 = 10 cents
#2 a nickel plus the minimum to make 11-5 =  6 cents
#3 a dime   plus the minimum to make 11-10 = 1 cent

options 1 and 3 result in 2 coins to make 11 cents, while option 2 results in 3 coins (2 nickels and 1 penny)

def makeChange(coinValues, change):
	minCoins = [0] * (change+1)
	for cents in range(change+1):
		coinCount = cents
		for j in [c for c in coinValues if c <= cents]:
			if minCoins[cents-j] + 1 < coinCount:
				coinCount = minCoins[cents-j] + 1
		minCoins[cents] = coinCount
	return minCoins[change]
			
5.1 Sorting and Searching

Implement sequential and binary search
Use hashing as a search technique
Implement the Map abstract data type

5.2 Searching is the process of finding a particular item in a collection of items. A search typically returns True or False

  3 in [1, 3, 5, 7, 9]
  => True

5.3 Sequential search is traversing a list and checking whether the desired item is in it

Regardless of whether the list is ordered or not, sequential search is O(n)

5.4 Binary search takes advantage of an ordered list's structure by discarding the half where we know the desired item is not located in every step

Binary search may be implemented as a recursive divide-and-conquer function

def search(lst, item):
	if len(lst) == 0:
		return False
		
	midpt = len(lst) // 2
	if lst(midpt) == item:
		return True
	else:
		if item < lst[midpt]:
			return search(lst[:midpt], item)
		else:
			return search(lst[midpt+1:], item)

in each comparison, half the list is eliminated

comparisons  number of items left
  1			    n/2
  2				n/4
  3				n/8
  ...			...
  i 			n/2^i

n/2^i = 1

solving for i, we get i = log_2(n)

The cost of sorting should be considered; sometimes it is better to use a sequential search

5.5 Hashing

(techopedia) Hashing is a one-way function that generates a value from a string (or number) using a mathematical function.

With hashing, we can build a data structure that can be searched in O(1) time

A hash table is a collection of items which are stored in such a way as to make it easy to find them later. Each position is called a slot.

The mapping between an item and the slot where it belongs in the hash table is called the hash function.

The remainder method takes an item and divides it by the table size
	h(item) = item % 11

The load factor measures how full the hash table is. L = num. of items / table size

When two items have the same hash value, it is called a collision.

5.5.1 Hash functions

A perfect hash function maps each item to a unique slot.

The goal is to create a hash function that minimizes the number of collisions, is easy to compute, and evenly distributes the items.

The folding method divides the item into equal-size pieces, adds them, and keeps a remainder corresponding to the table size

The mid-square method squares the item and takes a portion of the result, such as the middle two digits

Strings may be converted to lists of character codes (integers)

Since anagrams would have the same hash value, we can give each character a different weight based on its position.

5.5.2 Collision resolution is the systematic method for placing the second item in the hash table when there is a collision.

Open addressing is moving sequentially (possibly circularly to the beginning of the hash table) until we encounter an empty slot. This technique is called linear probing.

A disadvantage to linear probing is clustering

Looking for another slot is called rehashing

newhashvalue = rehash(oldhashvalue)
rehash(pos) = (pos + 3) % sizeoftable  # plus-3

To ensure all slots will be visited by the skip, table sizes should be a prime number.

Quadratic probing uses successive perfect squares (1, 4, 9, 16, etc.) for the skip value

Chaining allows many items to exist in the same slot

 0  1  2  3  4
[ ][ ][ ][ ][ ]
 |        |
 v        v
 44       25
 |
 v
 77

5.5.3 Implementing the Map abstract data type

The dictionary is an associative data type where you can store key-data pairs. The key is used to look up the associated data value. This idea is referred to as a map.

The map structure is an unordered collection of associations between a key and a data value. Keys are unique.

Map() creates an empty map collection.
put(key, val) adds a new key-value pair
get(key) returns the associated value or None if it is not found
del map[key]
len() returns the number of key-value pairs
key in map tests if the given key is in the map

class HashTable:
	def __init__(self):
		self.size = 11  # a prime number
		self.slots = [None] * self.size  # holds keys
		self.data = [None] * self.size   # holds values

hashfunction implements the simple remainder method

	def hashfunction(self, key, size):
		return key % size

	def rehash(self, oldhash, size):
		return (oldhash + 1) % size

put assumes there will eventually be an empty slot unless the key is already present. If the key is present, the new data replaces the old data.

get computes the initial hash value. If the value is not in the initial slot, rehash is used. 'startslot' is defined to ensure the search terminates.

def __getitem__(self, key):
	return self.get(key)

def __setitem__(self, key, data):
	self.put(key, data)

5.5.4 Analysis of Hashing

In the best case, searching is O(1). However, due to collisions, the performance depends on L, the load factor.

                Successful search          Unsuccessful search
With chaining   1 + (L / 2)                L
Linear probing  (1/2) * (1 + (1 / (1-L)))  (1/2) * (1 + (1 / (1-L)^2))

5.6 Sorting is the process of placing elements from a collection in some kind of order

The efficiency of a sorting algorithm is related to the number of items being processed.

For small collections, a complex sorting method may have a too high of an overhead.

The total number of comparisons is the most common way to measure the efficiency of a sort procedure. In addition, the total number of exchanges is also important.

5.7 Bubble sort makes n-1 passes comparing two adjacent items and exchanging them if they are out of order.

Unlike most sorting algorithms, bubble sort can recognize when a list is already sorted (if there were no exchanges made)


54 26 93 17 77   Exchange
'---'

26 54 93 17 77   Do not exchange
   '---'

26 54 93 17 77   Exchange
      '---'

26 54 17 93 77   Exchange
         '---'

26 54 17 77 93  93 in the right place

Bubble sort is O(n^2)

5.8 Selection sort makes only one exchange in each of the n-1 passes

In each pass, the largest item is found and swapped with the value in its correct place

26 54 (93) 17 77   93 is largest
       `-------^

26 54 (77) 17 93   77 is largest
       `----^

26 (54) 17 77 93   54 is largest
    `----^

(26) 17 54 77 93   26 is largest
`-----^

17 26 54 77 93     List is sorted

Selection sort is also O(n^2) but makes much fewer exchanges than bubble sort.

5.9 Insertion sort maintains a sorted sublist in the lower positions. Each new item is inserted back in the right place of the sorted sublist. We shift to the right every item of the sorted sublist that is larger than the current item

sorted sublist  current item
,------------,  v
17 26 54 77 93 31 44 20 55
             `\.
17 26 54 77 31 93 44 20 55
          `\.
17 26 54 31 77 93 44 20 55
       `\.
17 26 31 54 77 93 44 20 55
`---------------'
Now these are sorted

Insertion sort is O(n^2) but shifting is typically more efficient than exchanging.

5.10 Shell sort is also called "diminishing increment sort"

The original list is broken into smaller sublists, each of which is sorted with insertion sort. These sublists are made up of items i items apart (the gap)

After sorting the sublists, the items are much closer to their final sorted positions. This final sort is a standard insertion sort

It looks like shell sort is not much better than the standard insertion sort, but in practice, performance is between O(n) and O(n^(3/2)). This is because the final sort makes many fewer comparisons than the standard insertion sort.

5.11 Merge sort uses a divide-and-conquer strategy. It is a recursive algorithm that continually splits a list in half

Once such sublists are reduced to a single item, they are merged to form the sorted list. The larger and larger lists are merged

      [][][][][][]
      ,----''-----.        Split
   [][][]       [][][] 
   /    \       /    \
  []   [][]    []   [][]
  .    /  \    .    /  \
  .   []  []   .   []  []
  .   .    .   .   .    .
  .   .    .   .   .    .  then merge
    ..[][][]     ..[][][]
	   .            .
        [][][][][][]						   

a merge operation on a list of size n requires n operations. The list is divided log n times. So, merge sort is O(n log n)

Merge sort requires extra space to store the two halves

5.12 Quick sort is another divide-and-conquer algorithm. However, it does not require additional storage.

At first, a pivot value is chosen. Here, we will use the first item in the list.

The position where the pivot value ends up is the split point.

The partition process finds the split point and moves other values either to the left or right of the split point if they are less than or greater than the pivot, respectively.

  54 26 93 17 77 31 44 55 20
pivot `leftmark -> <- rightmark

move leftmark to the right until a value greater than the pivot is found

move rightmark to the left until a value less than the pivot is found

then exchange the two values

When rightmark < leftmark, the split point is found at the location of rightmark. We exchange the pivot value with rightmark

Now we recursively quick sort the items to the left of the split point and those to the right.

def quickSort(alist):
	quickSortHelper(alist, 0, len(alist) - 1)

def quickSortHelper(alist, first, last):
	if first < last:
		splitpoint = partition(alist, first, last)
		quickSortHelper(alist, first, splitpoint-1)
		quickSortHelper(alist, splitpoint+1, last)
	...

On average, quick sort is n log n, but in the worst case, where the splits are uneven, it is O(n^2)

The median of three technique chooses the median of the first, middle, and last items of the list as the pivot point.

6.1 Trees and Tree Algorithms

- What is a tree structure?
- how to implement a map data structure with trees
  - a list
  - classes and references
  - a recursive data structure
- to implement a priority queue using a heap

6.2 Examples of trees

Trees are hierarchical (structured in layers with more general things near the top (the root) and more specific things near the bottom (the leaves))

Example: biological classification of kingdom, phylum, class, ..., species. We can follow paths until we reach a creature's name at the very bottom

Children of one node are independent of children of another node.

Example: filesystem tree. A section of a tree is called a subtree

Example: webpage
         _____(html)_____
        /                \   
    (head)             _(body)_
   /      \           /   |    \
(meta)  (title)    (ul)  (h1)  (h2)
                   /||\
			     ...etc...

6.3 Tree Vocabulary and Definitions

Node: a fundamental part of a tree. Its name is the key. Additional data is called the payload.

Edge: connects two nodes to show that there is a relationship between them. Every node except the root is connected by exactly one incoming edge. A node may have any number of outgoing edges

Root: the only node that has no incoming edges

Path: an ordered list of nodes that are connected by edges, such as: Mammal -> Carnivora -> Felis -> cat

Children: the set of nodes that have incoming edges from the same node

Parent: a node is the parent of all nodes it connects to with outgoing edges

Sibling: nodes that are children of the same parent are siblings

Subtree: the set of nodes and edges comprised of a parent and all descendants of that parent.

Leaf node: a node with no children

Level: the level of a node is the number of edges on the path from the root node to it. The level of the root node is 0.

Height: the height of a tree is the maximum level of any node in the tree

Definition 1: A tree is a set of nodes and edges that connect pairs of nodes. A tree has these properties:

- one node is designated as the root node
- every node except the root is connected by an edge from exactly one other node, its parent
- a unique path traverses from the root to each node
- if each node has at most two children, it is a binary tree

Definition 2 (recursive): A tree is either empty or consists of a root and zero or more subtrees, each of which is also a tree. The root of each subtree is connected to the root of a parent tree by an edge.

6.4 List of lists representation - a three-element list

The first element is the value of the root node
The second element is the left subtree (another list)
The third element is the right subtree (another list)

['a', ['b', [], []], ['c', ['f', [], []], []]]  # it's recursive

The parts of the tree are accessed by standard list indexing: tree[0] is the value, tree[1] the left subtree and tree[2] the right subtree

# Binary Tree functions for creating, splicing, and accessing values. Classes are not created.

6.5 Nodes and references: a BinaryTree class that has attributes for the value and left and right subtrees

class BinaryTree:
	def __init__(self, rootObj):
		self.key = rootObj
		self.leftChild = None   # references to other BinaryTrees
		self.rightChild = None

For insertion, we must check if there is already a subtree:

	def insertLeft(self, newNode):
		if self.leftChild is None:
			self.leftChild = BinaryTree(newNode)
		else:
			t = BinaryTree(newNode)
			t.leftChild = self.leftChild
			self.leftChild = t

6.6 Parse Trees may be used to represent sentences and mathematical expressions.

         (Sentence)
        /          \
(Noun Phrase)  (Verb Phrase)
      |          |      |
(Proper Noun)  (Verb) (Noun)
      |          |      |
   (Homer)     (Hit)  (Bart)

     __(*)__
    /       \
  (+)       (-)  
  / \       / \
(7) (3)   (5) (2)

To build a parse tree, we first break up the expression string into a list of tokens:

(, ), operators and operands

When we read a left parenthesis, we start a new expression (new tree)

1) If we read '(', add a new node as the left child of the current node, and descend to this left child

2) If the token is an operator (+, -, /, *), set the root value of the current node to the operator and descend to the right child

3) If the token is a number, set the root value to the number and return to the parent

4) If the token is ')', go to the parent of the current node

To keep track of the parents, we can use a stack: whenever we descend to a child of the current node, we push the current node on the stack. To return to the parent, we pop the stack

To evaluate a parse tree, we recursively evaluate each subtree. The base case is a leaf node; in our case, a number that evaluates to itself. With operators, we apply them to the result of the left and right subtrees.

6.7 The common Tree Traversals are preorder, inorder, and postorder. A traversal is the visitation of the nodes.

preorder: visit the root node, then recursively do a preorder traversal of the left subtree and a preorder traversal of the right subtree

inorder: recursively traverse the left subtree, visit the root node, then traverse the right subtree

postorder: recursively traverse the left and right subtrees, then visit the root node

Preorder traversal is like reading a book front-to-back

def preorder(tree):
	if tree:
		print(tree.getRootVal())
		preorder(tree.getLeftChild())
		preorder(tree.getRightChild())

Evaluating a parse tree is done in postorder traversal.

Inorder traversal of a parse tree prints back the original expression without parentheses.

6.8 Priority Queues with Binary Heaps

A priority queue lets you dequeue items from the front, but enqueuing will place that item based on its priority (the logical order is determined by its priority). In effect, a priority queue is always sorted.

A binary heap allows enqueuing and dequeuing in O(log n) time.

We will implement a min heap (smallest key value is at the front). A max heap has the largest key value at the front.

6.9 Binary Heap operations

BinaryHeap() creates a new, empty binary heap
insert(k) adds a new item
findMin() returns the item with the minimum key value, leaving it in the heap
delMin() returns the minimum and removes it
isEmpty()
size()
buildHeap(alist) builds a new heap from a list of keys

6.10.1 Binary Heap Implementation - Structure

In order to guarantee logarithmic performance of a binary tree, it must be balanced (have roughly the same number of nodes in the left and right subtrees)

In our heap implementation we create a complete binary tree (each level has all its nodes). The exception is the bottommost level, filled from left to right

The heap is represented by a list, with an empty (zero) space in the 0th position. This is done so that the left child of a parent in position p is in 2p, and the right child in 2p+1. Given a node at position n, its parent is in n//2

6.10.2 The Heap Order Property

for every node x with parent p, the key in p is smaller than or equal to the key in x.

        __5__
       /     \
      9       11
     / \     /  \
    /   \   19  21
  14     18
 /  \   /
33  17 27

6.10.3 Heap operations

A binary heap (BinHeap) object has a list (with 0 as its 0th element) and a size attribute 'currentSize'

in inserting an item, we have to maintain the heap order property. If the new item is less than its parent, we swap them

'percUp' percolates (move gradually) the new item up as fas as it's needed.

delMin() returns the root and deletes it. The structure and order properties need to be restored.

To restore the structure, the last item is moved to the root position.

'percDown' swaps the root with its smallest child less than the root. We repeat the process until it ends up in a position where it is already less than both children.

To build a heap from a given list, we start in the middle: i = len(alist) // 2 (before adding the dummy, initial zero). Nodes past the halfway point are all leaves.

After adding the zero, we "percolate down" all nodes from i counting back to zero.

This procedure is O(n) and is the basis for the O(n log n) heapsort.

[Wikipedia] Heapsort is like an improved selection sort in that the minimal item is repeatedly found, and added to the result. Instead of the linear search in selection sort, heapsort uses the more efficient heap structure.

6.11 Binary Search Trees

So far we have used binary search on a list and hash tables to implement maps

A binary search tree is another structure that may be used to implement a map. It provides for efficient searching

6.12 Search Tree operations

This interface is very similar to a Python dictionary.

Map() creates a new, empty map
put(key, val) adds a new key-value pair or replaces an existing pair
get(key) returns the value stored in the map or None if it is not found
del map[key] deletes the key-value pair
len() returns the number of key-value pairs stored
key in map returns True if the key is in the map

6.13 Search Tree implementation

The bst property of a tree states that keys less than the parent are in the left subtree while keys greater than the parent are in the right subtree.

Because we must be able to create and work with an empty binary search tree, our implementation will use two classes: BinarySearchTree and TreeNode

BinarySearchTree keeps track of the size. TreeNode keeps track of its children, parent, key, and payload

put calls a recursive _put helper method if there's a root. Otherwise it installs a new node as the root.

myTree['abc'] = 23 is possible by overriding __setitem__.

def __setitem__(self, k, v):
    self.put(k, v)

get, _get, and __getitem__ are like _put and are recursive.

To allow 'k in bstree', override __contains__

Deleting a node must consider whether the node
- has no children (is a leaf) - simply delete it and remove the reference to it in its parent
- if the node has a single child, we promote the child to take the place of its parent
- to delete a node with two children, we must find a successor to take its place. It will have no more than one child, so we may remove it in the ways described above.

findMin() looks for the smallest item (the leftmost node), and the successor considers three cases:

1. if the node has a right child, then the successor is the smallest key in the right subtree.
2. if the node has no right child and is the left child of its parent, then the parent is the successor.
3. if the node is the right child of its parent and itself has no right child, then the successor to this node is the successor of its parent, excluding this node.

* the first condition is the only one that matters

yield allows us to create an iterator. It is used in generator functions

__iter__ overrides 'for x in something' an is recursive when using for x in ... in its definition

6.14 Search Tree Analysis

The put method's limiting factor on its performance is the height of the binary search tree.

If the keys are added in random order, the height of the tree will be around log_2 n

The total number of nodes of a perfectly balanced tree is 2^(h+1) - 1, where h is the height of the tree

If a binary search tree is perfectly balanced, the worst-case performance of put is O(log_2 n)

If keys are inserted in sorted order, performance is O(n) (degenerates to a linked list)

get, del and in are limited similarly because they may fail to find the desired key.

6.15 Balanced Binary Search Trees

An AVL tree (named after G. M. Adelson-Velskii and E. M. Landis) remains balanced at all times.

Each node keeps track of a balance factor

balancefactor = height(leftSubtree) - height(rightSubtree)

An AVL tree is either left-heavy (balancefactor < 0), right-heavy, or perfectly in balance.

6.16 AVL tree performance

The maximum number of nodes of the most unbalanced trees under these rules are:

N_h = 1 + N_h-1 + N_h-2 (like the Fibonacci sequence)

F_i = F_i-1 + F_i-2
Fi ~= Phi^i / sqrt(5)
N_h = (Phi^(h+2) / sqrt(5)) - 1
h ~= 1.44 log N_h

As a result, searching in an AVL tree is O(log n)

6.17 AVL tree implementation

When adding a new key, it will be added as a leaf node. We must recursively update the balance factor of its parent and ancestors.

Once a subtree has a balance factor of zero, the balance of its ancestors does not change

The AVLTree class is a subclass of BinarySearchTree.

updateBalance, self.rebalance, rotateLeft, rotateRight


   _(B)_                          _(D)_
  /     \       rotateLeft       /     \
[A]     (D)    ----------->    (B)     [E]
       /   \                  /   \
     [C]   [E]              [A]   [C] 

To avoid an unbalancing of o  to  o, two rotations are needed.
                            \    /
							 o  o
							/    \
						   o      o

6.18 Summary of Map ADT Implementations

operation  Sorted List  Hash Table  BST   AVL Tree
 put       O(n)         O(1)        O(n)  O(log n)
 get       O(log n)     O(1)        O(n)  O(log n)
 in        O(log n)     O(1)        O(n)  O(log n)
 del       O(n)         O(1)        O(n)  O(log n)

7.1 Graphs - Objectives

A tree is a special kind of graph.
Examples: road networks, airline flights, sequence of classes in a university

7.2 Vocabulary and Definitions

A vertex (or node) is a fundamental part of a graph. Its name is the key, and additional info the payload

An edge connects two vertices to show a relationship between them. Edges may be one-way or two-way. If a graph only has one-way edges, it is a directed graph, or digraph

weight: edges may be weighted to show that there is a cost to go from one vertex to another. (ex. distances)

A graph G = (V, E) has a set of vertices V and a set of edges E. Each edge is a tuple (v, w). This tuple may have a weight as a third element.

path: a sequence of vertices that are connected by edges: w1, w2, ..., wn such that (w_i, w_i+1) is in E for all 1 <= i <= n-1

cycle: in a directed graph, a cycle is a path that starts and ends at the same vertex. A graph with no cycles is called an acyclic graph.

A directed acyclic graph is called a DAG. Several important problems may be solved if they can be represented as a DAG

7.3 The Graph Abstract Data Type

Graph() creates a new, empty graph
addVertex(vert)
addEdge(fromVert, toVert) = directed edge
addEdge(fromVert, toVert, weight) = weighted, directed edge
getVertex(vertKey)
getVertices()
v in g

7.4 Adjacency Matrix

From  To ->
 |
 v
     v0  v1  v2  ...    
 vo       5
 v1           4
 v2   1
 ...

A value at the intersection of row v and column w indicates an edge

Two connected vertices are adjacent. The number of edges needed to fill the matrix is |V|^2

An adjacency matrix is usually inefficient because they will be sparse (most cells are empty)

7.5 Adjacency List

Keep a master list of all the vertices in the Graph object and then each vertex object in the graph maintains a list of the other vertices it is connected to.

In our implementation, we will use a dictionary where the keys are the vertices and values the weights

+-----------------+
| Graph           |
|  +----------+   |
|  | VertList |   |
|  |          |   |
|  | [v0| ]---------> id = "v0", adj = {'v1': 5, 'v5': 2}
|  | [v1| ]---------> id = "v1", adj = {'v2': 4}
|  | [v2| ]---------> id = "v2", adj = {}
|  | ...      |   |
|  +----------|   |
| numVertices = 6 |
+-----------------+

7.6 Adjacency List Implementation

The 'Graph' class holds the master list of vertices
The 'Vertex' clsss uses a dictionary to keep track of the vertices to which it is connected, and the weight of each edge

class Vertex:
	def __init__(self, key):
		self.id = key
		self.connectedTo = {}

	addNeighbor()
	getConnections()
	getId()
	getWeight()

class Graph:
	def __init__(self):
		self.vertList = {}

	addVertex()
	addEdge()
	getVertex()
	getVertices()
	__contains__
	__iter__

7.7 The word ladder problem

Transform "FOOL" to "SAGE" by changing one letter at a time. Find the smallest number of transformations needed

7.8 Building the word ladder graph

Comparing a word to every other word is infeasible (O(n^2))

Instead, we use buckets such as '_OPE' and place in it words that match (_ is a wildcard).
'_OPE' -> POPE, ROPE, MOPE, COPE, NOPE, ... These words are all connected

Now we create a vertex for each word and edges between all vertices found in each bucket

7.9 Implementing Breadth-First Search (BFS)

Given a graph G and a starting vertex S, BFS explores edges in G to find all the vertices in G for which there is a path from S.

It finds all vertices that are a distance k from s before it finds any that are a distance k+1

All vertices start as white. A gray vertex still has unexplored vertices (when a vertex is initially discovered it is colored gray). A black vertex is completely explored

BFS uses a queue to decide which vertex to explore next

The extended Vertex class used in BFS includes distance, predecessor, and color

BFS begins at the starting vertex 'start' and colors it gray. It is placed in the Queue, and we systematically explore vertices in the front of the queue.

'nbr' = new, unexplored vertex

1. nbr is colored gray
2. the predecessor of nbr is set to 'currentVert'
3. nbr's distance is set to currentVert + 1
4. nbr is enqueued, scheduling it for further exploration

7.10 Breadth-First Search Analysis

The while loop is O(V) because each node is visited only once

The for loop is O(E) because it is run once when the node is dequeued. Combined, it is O(V+E)

In the worst case, if the graph was a long chain, traversing all the vertices would be O(V). There is also the time needed to build the graph

7.11 The Knight's Tour Problem

Find a sequence of moves that allow the knight to visit every square on the board exactly once. Such a sequence is called a tour.

7.12 Building the Knight's Tour Graph

Each square on the chessboard is a node in the graph and each legal move, an edge.

'genLegalMoves' and 'legalCoord' generate edges

7.13 Implementing Knight's Tour

The search algorithm used to solve the knight's tour problem is depth-first search (DFS)

DFS explores one branch of the search tree as deeply as possible.

There are two approaches, one that marks nodes as visited and a more general approach

When a dead end is reached, DFS backs up the tree to the next deepest vertex that allows a legal move

'knightTour' is recursive. It takes four parameters:

n (current depth)
path
u (the vertex we wish to explore)
limit (the number of nodes in the path)

Its recursive nature implies we are using a stack to help us with backtracking

unvisited vertices are colored white and visited ones are colored gray

upon reaching a dead end, we change the vertex's color back to white

7.14 Knight's Tour analysis

The algorithm described is O(k^N), where N is the number of squares on the board and k is a small constant

A heuristic (common sense or practical knowledge to speed up an algorithm) is to visit squares with the fewest available moves first

If we visit easy-to-reach squares early, later the knight gets stranded and unable to reach the other side of the board

7.15 General Depth-First Search

The Knight's Tour is a special case of DFS where we create the deepest depth-first tree without branches

General DFS's goal is to search as deeply as possible, connecting as many nodes as possible, and branching where necessary

DFS may create many trees; we call them a depth-first forest

In addition to predecessor links, DFS vertices make use of discovery and finish times

discovery time: number of steps before a vertex is first encountered

finish time: number of steps before a vertex is colored black

DFSGraph inherits from Graph, adding a 'time' instance variable

We iterate over all the nodes to make sure no vertices are left out of the depth-first forest

dfsvisit calls itself recursively, so there is an implicit stack that is not visible in the code

The starting and finishing times for each node display the parenthesis property: all children of a particular node in the depth-first tree have a later discovery time and earlier finish time than their parent.

7.16 Depth-First Search Analysis

The loops in DFS run in O(V) and dfsvisit is called at most once per edge, or O(E). The total time is O(V+E)

7.17 Topological sorting

Example: cooking pancakes. A directed graph indicates which steps need to be completed first (mixing ingredients, heating griddle, turning pancake)

To decide the precise order in which we should do the steps we turn to a graph algorithm called the topological sort

A topological sort takes a directed acyclic graph (DAG) and produces a linear ordering of all its vertices such that if the graph G contains an edge (v, w) then the vertex v comes before the vertex w in the ordering.

Other examples: project schedules, database query optimization

The topological sort algorithm is:

1. call DFS(g) for the graph g, to compute the finish times for each of the vertices
2. store the vertices in a list in decreasing order of finish time
3. return the ordered list

7.18 Strongly Connected Components

Search engines exploit the fact that web pages form a very large directed graph. A page is a vertex, and a hyperlink is an edge

One graph algorithm that can help find clusters of highly interconnected vertices in a graph is called the Strongly Connected Components (SCC) algorithm

A strongly connected component C of a graph G is the largest subset of vertices C in V such that for every pair of vertices v, w in C we have a path from v to w and a path from w to v

Once the strongly connected components have been identified, we can combine all the vertices in one strongly connected component into a single larger vertex.

The transposition of a graph G is the graph G^T where all the edges in the graph have been reversed (an edge from A to B becomes an edge from B to A)

The algorithm is:
1. call DFS(g) to compute the finish times for each vertex
2. compute g^T
3. call DFS(g^T) but in the main loop of DFS explore each vertex in decreasing order of finish time
4. each tree in the forest computed in step 3 is a strongly connected component. Output the vertex ids for each vertex in each tree in the forest to identify the component

This book does not have the SCC algorithm's code

7.19 Shortest Path Problems

Example: a browser request for a web page
traceroute shows the routers between the requester and the destination server

A network of routers may be represented as a weighted graph

7.20 Dijkstra's Algorithm is used to determine the shortest path

The 'dist' instance variable in the Vertex class is used to store the current total weight of the smallest weight path from the start to the vertex in question

The algorithm iterates once for every vertex; however, the order is controlled by a priority queue

The value used in the priority queue is dist. Initially, dist is set to infinity or any large value larger than any real distance in the problem.

The predecessors are set with vert.setPred(w)

The priority queue used has the additional 'decreaseKey' method that updates a vertex's dist, moving it toward the front of the queue.

If a negative weight existed, the algorithm would never exit

Real Internet routing algorithms do not use Dijkstra's algorithm because it requires knowing the complete representation of the graph.

In practice, an algorithm such as "distance vector" routing is used

7.21 Analysis of Dijkstra's Algorithm

Building the priority queue is O(V). Calling 'delMin' takes O(log V) time, altogether taking O(V log V) time.

The for loop is executed once for each edge, and decreaseKey is called in this loop, taking O(E log V) time. Combined, the running time is O((V+E) log(V))

7.22 Prim's Spanning Tree Algorithm

This algorithm is used to solve the broadcast problem (transferring a piece of information to anyone who is listening)

Examples: Internet Radio, online game messaging

The solution to this problem is constructing a minimum weight spanning tree, T, for a graph G = (V, E)

T is an acyclic subset of E that connects all the vertices in V such that the sum of the weights of the edges in T is minimized

From the starting vertex, each following vertex forwards the message to any neighbor that is part of the spanning tree.

Prim's algorithm is a greedy algorithm. It chooses the cheapest next step at each step (the edge with the lowest weight)

pseudocode:
	while T is not yet a spanning tree:
		find an edge that is safe to add
		add the new edge to T
		
A safe edge connects a vertex in the spanning tree to a vertex that is not in the spanning tree. We do not want to create cycles

Like Dijkstra's algorithm, a priority queue is used.
