Preface
-------
a deep neural net is a simplified model of our cerebral cortex, composed of a stack of layers of neurons.

the goal is implementing programs capable of learning from data.

frameworks used:
- Scikit-Learn (implements many machine-learning algorithms)
- TensorFlow (runs neural networks)
- Keras (a deep learning API)

code in this book can be run in Google Colab

Part I covers:

- what is machine learning?
- the steps in a typical machine learning project
- fitting a model to data
- optimizing a cost function
- handling, cleaning, and preparing data
- selecting and engineering features
- selecting a model and tuning hyperparameters using cross-validation
- challenges in machine learning (underfitting, overfitting, bias/variance trade-off)
- the most common algorithms:
  - linear and polynomial regression
  - logistic regression
  - k-nearest neighbors
  - support vector machines (SVMs)
  - decision trees
  - random forests
  - ensemble methods
- reducing the dimensionality of training data (fight the "curse of dimensionality")
- unsupervised learning techniques:
  - clustering
  - density estimation
  - anomaly detection

Part II covers:

- what are neural nets?
- building and training neural nets using TensorFlow and Keras
- the most important neural net architectures:
  - feedforward neural nets for tabular data
  - convolutional nets for computer vision
  - recurrent nets and long short-term memory (LSTM) nets for sequence processing
  - encoder-decoders and transformers for natural language processing (NLP) and more
  - autoencoders
  - generative adversarial networks (GANs)
  - diffusion models for generative learning
- how to build an agent (such as a bot) that can learn strategies from trial and error, using reinforcement learning
- loading and preprocessing large amounts of data
- training and deploying TensorFlow models at scale

Ch. 1 (p. 23)
-------------
types of learning:
- supervised vs. unsupervised
- online vs. batch
- instance-based vs. model-based

machine learning is the science of programming computers so they can learn from data

when a program is given more "experience" (new data), it "learns" if its measured performance improves

the examples used to learn is the training set. it is made up of training instances (samples)

the part that learns and makes predictions is called the model

data mining: digging into large amounts of data to discover hidden patterns

examples of ML applications:
- image classification typically performed using convolutional neural networks (CNNs)
- semantic image segmentation (detecting tumors in brain scans), typically using CNNs or transformers
- classifying news articles using natural language processing (NLP), recurrent neural networks (RNNs), CNNs, or transformers
- summarizing long documents
- creating chatbots and personal assistants
- forecasting revenue, based on performance metrics, using any regression model (linear, polynomial, support vector machine, random forest, artificial neural network)
- speech recognition (RNNs, CNNs, transformers)
- anomaly detection (credit card fraud), using isolation forests, Gaussian mixture models, or autoencoders
- segmenting clients based on their purchases (clustering), using k-means or DBSCAN
- representing a complex dataset (data visualization) using dimensionality reduction techniques
- recommending a product (recommender system based on an artificial neural network)
- building an intelligent bot (reinforcement learning, RL)

supervised learning: the training set includes the desired solutions or labels. a typical task is classification

regression: predicting a target numeric value, given a set of features (price of a car)

logistic regression: outputs a probability of an item belonging to a given class

"label" is more often used in classification, while "target" is more used in regression

features are also called predictors or attributes

unsupervised learning: training data is unlabeled.

a clustering algorithm may be run to detect groups of similar items

visualization is also common in unsupervised learning: a 2D or 3D representation can be plotted

dimensionality reduction: simplifying the data without losing too much information. one way is to merge several correlated features into one.

anomaly detection: does a new item look like existing ones?

novelty detection: is the new item different from all instances in the training set?

association rule learning: given a large amount of data, discover interesting relations between attributes

semi-supervised learning: some instances are labeled

self-supervised learning: generating a fully labeled dataset from a fully unlabeled one
example: given a large dataset of unlabeled images, randomly mask a small part of each image and train a model to recover the original image

transfer learning: transferring knowledge from one task to another

in reinforcement learning, the agent can observe the environment, select actions, and get reward or penalties based on the choice. it must then learn by itself what is the best strategy (policy)

batch learning: the system is incapable of learning incrementally. it must be trained using all the available data. this is offline learning.

model rot (or data drift): the diminishing performance of a model due to changes in the world.

online learning: you train the system incrementally by feeding it data instances sequentially, individually or in mini-batches

learning rate: how fast they should adapt to changing data. a high learning rate means the system rapidly adapts to new data, but also quickly forgets old data. with a low learning rate, the system will have more inertia

out-of-core learning is usually done offline, so "online learning" is a confusing term. it's better described as "incremental learning"

online learning system are subject to errors (bugs) or abuse (someone trying to game the system) so it's important to monitor the data and performance, and be able to revert to a previous state if needed.
